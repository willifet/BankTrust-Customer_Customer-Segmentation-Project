


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


#Load dataset

data = pd.read_csv("../Dataset/bank_data_C.csv")
data.head()


data.describe(include = 'all')


data.info()


data.dtypes





data.head(5)


#Checking for missing value

data.isnull().sum()


#Convert CustomerDOB and transaction date to datetime

data["CustomerDOB"] = pd.to_datetime(data["CustomerDOB"])
data["TransactionDate"] = pd.to_datetime(data["TransactionDate"], format = '%d/%m/%y')


#Checking for unique value
data["TransactionDate"].unique()


#Calculating the age of every customer by subtracting the transaction date from DOB
#We first create a new column for age

def calculate_age(data):
    data['Age'] = data["TransactionDate"].dt.year - data["CustomerDOB"].dt.year
    return data


data = calculate_age(data)
data.head()


#Observe and correct Customers negative age
data[data["Age"] < 0]["CustomerDOB"]


#Define a function to correct a negative age by adjusting the date of the year

def adjust_year(date):
    if date.year>2016:
        date = date.replace(year= date.year - 100)
    return date


data["CustomerDOB"] = data["CustomerDOB"].apply(adjust_year)


data = calculate_age(data)


data.head()


#To observe age distribution 
plt.figure(figsize=(8,6))
sns.set_palette('inferno')
sns.histplot(data['Age'], bins = 10, kde = False)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age distribution')
plt.show()


#examine the wrong entry
data[data["Age"] > 100]["CustomerDOB"].unique()


#Removing the outliers as a result of wrong entries
#Define function to fix outliers in age

def replace_age_outliers(data):
    DOB_threshold = 1900
    age_outliers = data[data["CustomerDOB"].dt.year < DOB_threshold].index

    mean_DOB = data[~data.index.isin(age_outliers)]["CustomerDOB"].mean()

    data.loc[age_outliers, "CustomerDOB"] = mean_DOB

    return data
    


data = replace_age_outliers(data)
data = calculate_age(data)


#To observe age distribution 
plt.figure(figsize=(8,6))
sns.histplot(data['Age'], bins = 10, kde = False)
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Age distribution')
plt.show()


#To observe Gender column

data["CustGender"].value_counts()


#Replace T in gender with M
data["CustGender"] = data["CustGender"].replace('T', 'M')


data["CustGender"].value_counts()


#To observe location column
data["CustLocation"].value_counts()


#To observe Account balance column
data["CustAccountBalance"].sort_values(ascending = True)


#observe transaction amount

data[data["TransactionAmount (INR)"]== 0].value_counts().sum()


#Drop all the rows with zero transactions

data.drop(data[data["TransactionAmount (INR)"] == 0].index.tolist(), axis = 0, inplace = True)


data[data["TransactionAmount (INR)"]== 0].value_counts().sum()


data.head()





#Checking for customer unique ID
data["CustomerID"].nunique()


data.shape


#plot a distribution for data across the unique transaction date
#based on the graph we only have 3 months worth of data

plt.figure(figsize=(8,6))
sns.histplot(data["TransactionDate"], bins = 3, kde = False)
plt.xlabel("Transaction Date")
plt.ylabel("Frequency")
plt.title("Transaction date distribution")
plt.show()


#Checking the distribution for Gender with pie chart

plt.figure(figsize=(8,8))
gender_count = data["CustGender"].value_counts()
plt.pie(gender_count, labels = gender_count.index, autopct= '%1.1f%%', startangle = 180)
plt.title("Pie chart of gender")
plt.show()





data.head(5)





#To understand how frequent a customer transact 
#To obtain a maximum day 
day = data["TransactionDate"].max()


day


recency = data.groupby(['CustomerID']).agg({"TransactionDate": lambda x: ((day - x.max()).days) +1})


recency.head()





#Calculating a transaction ID for every customer

frequency = data.drop_duplicates(subset = "TransactionID").groupby(["CustomerID"])[["TransactionID"]].count()


frequency.head(5)





#To get the total sum of all the transaction amount made by every customer

monetary = data.groupby("CustomerID")[["TransactionAmount (INR)"]].sum()


monetary.head(5)





#Concatenate Every individual table created to create overall RFM table
RFM_Table = pd.concat([recency, frequency, monetary], axis = 1)


RFM_Table.head()


#Rename column headings 
RFM_Table = RFM_Table.rename(columns = {"TransactionDate": "Recency", "TransactionID": "Frequency", "TransactionAmount (INR)": "Monetary"})
RFM_Table.head()





fig, axes = plt.subplots(1,3, figsize= (15,5))

columns = ["Recency", "Frequency", "Monetary"]
for i, col in enumerate(columns):
    axes[i].hist(RFM_Table[col], bins= 10, color = 'skyblue', edgecolor = 'black')
    axes[i].set_title(col)
    axes[i].set_xlabel("Days" if col == "Recency" else "count" if col == "Frequency" else "INR")
    axes[i].set_ylabel(col)

plt.tight_layout()
plt.show()
    


plt.figure(figsize=(8,6))
plt.hist(RFM_Table['Monetary'], bins = np.logspace(0,5,20), color = 'skyblue', edgecolor = 'black')
plt.title("Monetary value")
plt.xlabel('monetary value (log scale)')
plt.ylabel('frequency (log scale)')
plt.xscale('log')
plt.yscale('log')
plt.show()


RFM_Table.corr()





RFM_Table.head()


#Calculate the quartile for each column
quantile = RFM_Table[["Recency", "Frequency", "Monetary"]].quantile(q = [0.25, 0.5, 0.75]).to_dict() 


quantile


RFM_Table["Frequency"].value_counts()


#Creating a custom system for frequency

def assign_R_score(x, feature):
    if x <= quantile[feature][0.25]:
        return 4
    elif x <= quantile[feature][0.5]:
        return 3
    elif x <= quantile[feature][0.75]:
        return 3
    else:
        return 1

def assign_M_score(x, feature):
    if x <= quantile[feature][0.25]:
        return 1
    elif x <= quantile[feature][0.5]:
        return 2
    elif x <= quantile[feature][0.75]:
        return 3
    else:
        return 4


def custom_frequency_score(x):
    if x <=3:
        return x
    else:
        return 4


RFM_Table["R_score"] = RFM_Table["Recency"].apply(lambda x: assign_R_score(x, "Recency"))

RFM_Table["F_score"] =  RFM_Table["Frequency"].apply(custom_frequency_score)

RFM_Table["M_score"] = RFM_Table["Monetary"].apply(lambda x: assign_M_score(x, "Monetary"))


RFM_Table


#Creating a total score that will be a combination of all 3 scores

RFM_Table["RFM_Score"] = RFM_Table[["R_score", "F_score", "M_score"]].sum(axis=1)


RFM_Table.head(5)


#Creating RFM Group to understand the segment

RFM_Table["RFM_Group"] = RFM_Table["R_score"].astype(str) + RFM_Table["F_score"].astype(str) + RFM_Table["M_score"].astype(str)


RFM_Table.head()





#Visualize the distribution along the RFM_Score

plt.figure(figsize = (15, 5))
sns.countplot(x = RFM_Table["RFM_Score"])





def assign_segments(x):
    if x <= 5:
        return "low"
    elif x <= 9:
        return "medium"
    else:
        return "high"


RFM_Table["segments"] = RFM_Table["RFM_Score"].apply(lambda x: assign_segments(x))


RFM_Table.head()


#Visualize the distribution along the segments

plt.figure(figsize = (10, 5))
sns.countplot(x = RFM_Table["segments"])





if "weighted Score" in RFM_Table.columns:
    RFM_Table.drop("weighted Score", axis=1, inplace=True)

RFM_Table["weighted score"] = (RFM_Table['R_score'] * 2) + (RFM_Table['F_score'] * 1) + (RFM_Table['M_score'] * 1)


RFM_Table.head()


#Visualize weighted_score

plt.figure(figsize = (10, 5))
sns.countplot(x = RFM_Table["weighted score"])


#Creating RFM Weighted segment

RFM_Table["weighted segment"] = RFM_Table["weighted score"].apply(lambda x: assign_segments(x))


RFM_Table.head()


#Visualize weighted_segment

plt.figure(figsize = (10, 5))
sns.countplot(x = RFM_Table["weighted segment"])








import sklearn
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import yellowbrick
from yellowbrick.cluster import KElbowVisualizer
from sklearn.decomposition import PCA






